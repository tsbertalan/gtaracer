{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record gameplay.\n",
    "Call recorder.stop() to interrupt recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gta.recording.unified\n",
    "recording = gta.recording.unified.UnifiedRecorder(\n",
    "    includeKeyboard=False,\n",
    "    gamepadPeriod=.001, visionPeriod=.05\n",
    ")\n",
    "recording.start()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Images are ... ', end='')\n",
    "while True:\n",
    "    X = recording.xrecorder.results\n",
    "    print('%.2fGB ' % (X.size * X.dtype.itemsize / 1024 / 1024 / 1024,), end='')\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recording.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = recording.xrecorder.results\n",
    "print('%.2fGB ' % (X.size * X.dtype.itemsize / 1024 / 1024 / 1024,), end='')\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = recording.save(compressed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.69e3/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved['Y'].size / 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes about 4 times as long if compression is indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and use data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gta.utils, gta.eventIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keepEids = [0, 2, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fpath = os.path.join(gta.utils.home, 'data', 'gta', 'UnifiedRecorder-1509210169.3261402.npz')\n",
    "\n",
    "try:\n",
    "    saved.keys()\n",
    "except NameError:\n",
    "    # Data not loaded.\n",
    "    saved = np.load(fpath)\n",
    "DT = np.diff(saved['T'])\n",
    "X = saved['X'][1:]\n",
    "# Only keep the first 6 features (gamepad; leave out buttons)\n",
    "Y = saved['Y'][1:][:, keepEids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data is too big for this\n",
    "# X = (X.astype('float32') - 128) / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(mat):\n",
    "    return (mat.astype('float32') - 127.5) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the distribution of outputs look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for col, eid in enumerate(keepEids):\n",
    "    label = gta.eventIDs.eids2names[eid]\n",
    "    ax.hist(Y[:, col], label=label, alpha=.2, normed=True, bins=32)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a randomly-chosen image look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imsh(im):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(im.astype('uint8'))\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsh(X[402]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the average image look like??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avImg = np.sum(X, axis=0) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsh(avImg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dark road in the middle, hillside on the right, a constant HUD, a faint overlay of a parking lot at the very end, and ... Task Manager at bottom right. Should have taken that out. :/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = int(len(DT) * .8), int(len(DT) * .9)\n",
    "\n",
    "def s(a, b):\n",
    "    return X[a:b], Y[a:b], DT[a:b]\n",
    "\n",
    "X_train, Y_train, DT_train = s(0, splits[0])\n",
    "X_test, Y_test, DT_test = s(splits[0], splits[1])\n",
    "X_valid, Y_valid, DT_valid = s(splits[1], -1)\n",
    "X_train.shape, X_test.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(DT_train)\n",
    "n_test = len(DT_test)\n",
    "n_valid = len(DT_valid)\n",
    "n_train, n_test, n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gta.nn\n",
    "reload(gta.nn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, *image_shape), name='images')\n",
    "y = tf.placeholder(tf.float32, (None, 3), name='gamepad_axes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arch(gta.nn.ConvNet):\n",
    "    \n",
    "    def _addConv2d(self, *args, **kwargs):\n",
    "        out = super(self.__class__, self)._addConv2d(*args, **kwargs)\n",
    "        print(out.shape)\n",
    "        return out\n",
    "    \n",
    "    def __call__(self, x, name='predictions'):\n",
    "        td = self._addConv2d\n",
    "        fc = self._addFc\n",
    "        #self.keep_prob = tf.placeholder_with_default(.5, shape=())\n",
    "        \n",
    "        x = td(x, (8, 8, self.c, 12), padding='SAME', pooling=False)\n",
    "        x = td(x, (8, 8, int(x.shape[-1]), 12), padding='VALID')\n",
    "        #x = tf.nn.dropout(x, self.keep_prob)\n",
    "        \n",
    "        x = td(x, (3, 3, int(x.shape[-1]), 16), padding='SAME')#, pooling=False)\n",
    "        x = td(x, (3, 3, int(x.shape[-1]), 16), padding='VALID')\n",
    "        \n",
    "        x = td(x, (3, 3, int(x.shape[-1]), 32), padding='SAME')#, pooling=False)\n",
    "        x = td(x, (3, 3, int(x.shape[-1]), 32), padding='VALID')\n",
    "        \n",
    "        x = td(x, (3, 3, int(x.shape[-1]), 64), padding='SAME')#, pooling=False)\n",
    "        x = td(x, (3, 3, int(x.shape[-1]), 64), padding='VALID')\n",
    "        \n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        \n",
    "        x = fc(x, (int(x.shape[-1]), 32))\n",
    "        x = fc(x, (int(x.shape[-1]), Y.shape[1]), name=name)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Arch(c=image_shape[-1])\n",
    "z = net(x)\n",
    "y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def breakBatches(X_data, Y_data, desc=None):\n",
    "    num_examples = len(X_data)\n",
    "    assert num_examples == len(Y_data)\n",
    "    for offset in tqdm.tqdm_notebook(list(range(0, num_examples, BATCH_SIZE)), desc=desc, unit='batch'):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], Y_data[offset:offset+BATCH_SIZE]\n",
    "        batch_x = normalize(batch_x)\n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data, sess=None, extraFeedDict={}, desc='validation', givePredictions=False):\n",
    "    if hasattr(net, 'keep_prob'):\n",
    "        extraFeedDict.setdefault(net.keep_prob, 1.0)\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    if sess is None: sess = tf.get_default_session()\n",
    "        \n",
    "    num = 0\n",
    "    den = 0\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for batch_x, batch_y in breakBatches(X_train, Y_train, desc=desc):\n",
    "        fd = {x: batch_x, y: batch_y}\n",
    "        fd.update(extraFeedDict)\n",
    "        run = lambda inp: sess.run(inp, feed_dict=fd)\n",
    "        \n",
    "        # TODO: verify that this reduce_sum operates over rows and columns.\n",
    "        zeval = run(z)\n",
    "        if givePredictions:\n",
    "            predictions.append(zeval)\n",
    "        num += run(tf.reduce_sum((y - zeval) ** 2))\n",
    "        den += batch_y.size\n",
    "        \n",
    "    if givePredictions:\n",
    "        return num / den, predictions\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Queue instructions.](http://ischlag.github.io/2016/11/07/tensorflow-input-pipeline-for-large-datasets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the untrained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a FIFOQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "raw_data = X_train\n",
    "raw_target = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "CHUNK_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queue_input_data = tf.placeholder(tf.float32, shape=(CHUNK_SIZE, *image_shape))\n",
    "queue_input_target = tf.placeholder(tf.float32, shape=[CHUNK_SIZE, Y_train.shape[1]])\n",
    "\n",
    "queue = tf.FIFOQueue(\n",
    "    capacity=50, dtypes=[tf.float32, tf.float32], \n",
    "    shapes=[image_shape, Y_train.shape[1:]]\n",
    ")\n",
    "\n",
    "enqueue_op = queue.enqueue_many([queue_input_data, queue_input_target])\n",
    "dequeue_op = queue.dequeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow recommendation:\n",
    "# capacity = min_after_dequeue + \n",
    "#      (num_threads + a small safety margin) * batch_size\n",
    "data_batch, target_batch = tf.train.batch(\n",
    "    dequeue_op, batch_size=BATCH_SIZE, capacity=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enqueue(sess):\n",
    "    under = 0\n",
    "    max = len(raw_data)\n",
    "    print(\"starting to write into queue\")\n",
    "    while True:\n",
    "        upper = under + CHUNK_SIZE\n",
    "        if upper <= max:\n",
    "            curr_data = raw_data[under:upper]\n",
    "            curr_target = raw_target[under:upper]\n",
    "            under = upper\n",
    "        else:\n",
    "            rest = upper - max\n",
    "            curr_data = np.concatenate((\n",
    "                raw_data[under:max], raw_data[0:rest]\n",
    "            ))\n",
    "            curr_target = np.concatenate((\n",
    "                raw_target[under:max], raw_target[0:rest]\n",
    "            ))\n",
    "            under = rest\n",
    "\n",
    "        sess.run(enqueue_op, feed_dict={queue_input_data: curr_data,\n",
    "                                        queue_input_target: curr_target})\n",
    "    print(\"finished enqueueing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y, z)\n",
    "learning_rate = .0001\n",
    "training_operation = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # start the threads for our FIFOQueue and batch\n",
    "    enqueue_thread = threading.Thread(target=enqueue, args=[sess])\n",
    "    enqueue_thread.isDaemon()\n",
    "    enqueue_thread.start()\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    nchunks = int(np.ceil(len(X_train) / CHUNK_SIZE))\n",
    "\n",
    "    MSEs = []\n",
    "    for iepoch in tqdm.tqdm_notebook(range(EPOCHS), unit='epoch', total=EPOCHS):\n",
    "        for b in tqdm.tqdm_notebook(\n",
    "            range(nchunks),\n",
    "            unit='chunk',\n",
    "            total=nchunks,\n",
    "            desc='epoch %d' % (iepoch+1,)\n",
    "        ):\n",
    "            # Fetch the data from the pipeline.\n",
    "            run_options = tf.RunOptions(timeout_in_ms=4000)\n",
    "            batch_x, batch_y = sess.run(\n",
    "                [data_batch, target_batch], options=run_options\n",
    "            )\n",
    "            batch_x = normalize(batch_x)\n",
    "            \n",
    "            # Do the training.\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        mse = evaluate(X_valid, Y_valid, sess)\n",
    "        MSEs.append(mse)\n",
    "        print('MSE=%s' % mse)\n",
    "    saver.save(sess, './gtaArch')\n",
    "\n",
    "    # Shutdown everything to avoid zombies.\n",
    "    # Once we are done with our input \n",
    "    # pipeline we should stop all running threads \n",
    "    # before closing the session.\n",
    "    sess.run(queue.close(cancel_pending_enqueues=True))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(MSEs)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('validation MSE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    extraFeedDict = {}\n",
    "    if hasattr(net, 'keep_prob'):\n",
    "        extraFeedDict.setdefault(net.keep_prob, 1.0)\n",
    "        \n",
    "    Z = {}\n",
    "    for k, Xd, Yd in zip(\n",
    "        ['train', 'test', 'valid'],\n",
    "        [X_train, X_test, X_valid],\n",
    "        [Y_train, Y_test, Y_valid],\n",
    "    ):\n",
    "#         zk = []\n",
    "#         for batch_x, batch_y in breakBatches(Xd, Yd, desc=k):\n",
    "#             fd = {x: batch_x, y: batch_y}\n",
    "#             fd.update(extraFeedDict)\n",
    "#             zk.append(sess.run(z, feed_dict=fd))\n",
    "#         Z[k] = zk\n",
    "        Z[k] = evaluate(Xd, Yd, desc=k, givePredictions=True)\n",
    "        print('MSE:', Z[k][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in Z:\n",
    "    Z[k] = Z[k][0], np.vstack(Z[k][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the network predict only zeros??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in Z:\n",
    "    print((Z[k][1] == 0).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i, eid in enumerate(keepEids):\n",
    "#     i = 0\n",
    "#     eid = keepEids[i]\n",
    "    label = gta.eventIDs.eids2names[eid]\n",
    "    color = ['red', 'blue', 'green'][i]\n",
    "    ax.plot(Y_train[:, i], label=label, color=color)\n",
    "    if i == 0: label='%s prediction' % label\n",
    "    else: label=None\n",
    "    ax.plot(Z['train'][1][:, i], color=color, linestyle='--', label=label)\n",
    "ax.legend(fontsize=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
